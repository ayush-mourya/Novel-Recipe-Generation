{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrmou4GQ_y_3"
      },
      "source": [
        "**Downloading, Installing & Importing Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C_dtB5nTylQ"
      },
      "outputs": [],
      "source": [
        "# Packages and Libraries Required for training the model and working with the dataset\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Packages and Libraries Required for implementing Utility/helper functions.\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import csv\n",
        "import sys\n",
        "import h5py\n",
        "import nltk\n",
        "import math\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import shutil\n",
        "import pickle\n",
        "import string\n",
        "import random\n",
        "import pickle\n",
        "import zipfile\n",
        "import pathlib\n",
        "import logging\n",
        "import argparse\n",
        "import platform\n",
        "import itertools\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from tqdm import tqdm, trange\n",
        "from nltk.stem.porter import *\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.manifold import TSNE\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.decomposition import PCA\n",
        "from transformers import GPT2Tokenizer\n",
        "from keras.layers import Bidirectional\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmlHQ0YTWA-c"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmIdh3Xf_8DH",
        "outputId": "e7d0a914-2262-4f13-8c80-596da0dba957"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTts_zswOGQ7"
      },
      "source": [
        "**Mounting Google Drive for importing the Data Files which will be used in the Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGpbSsbJOUu5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wQG5kGpVo2g"
      },
      "source": [
        "**Initializing the WordNet Lemmatizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKkMWtkvVuiv"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g4Kq-naWyfs"
      },
      "source": [
        "**Importing csv File which contains Unique Recipe IDs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "k4gNFM8KApMP",
        "outputId": "859e64c7-77c1-42e7-d2fd-71efd2581738"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/RecipeDB_v1/Recipe_correct_ndb_updated_v1.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH1zsAkGW5sV"
      },
      "source": [
        "**Fetching Unique Recipe IDs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGF5O1kZA3Wz"
      },
      "outputs": [],
      "source": [
        "recipeIds=list(df['recipe_no'].unique())\n",
        "print(recipeIds)\n",
        "recipeIdslistStringForm=list()\n",
        "for eachRecipeId in recipeIds:\n",
        "  recipeIdslistStringForm.append(str(eachRecipeId))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1OL5BvsXBcW"
      },
      "source": [
        "**Displaying the Number of Unique Recipe Ids**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jenBkWkVA8a4",
        "outputId": "d1d45d48-3afa-4c7d-bca2-98af4c36563f"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Unique Recipe Ids:\",len(recipeIdslistStringForm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_LfElkZXkKC"
      },
      "source": [
        "**Importing json File which contains Recipe Ids with thier Instructions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glWXzR14BCdc"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Final_Model_Rata2_Recipegen/recipe_db_data.json\") as data_file:\n",
        "    data = json.load(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofo_Tl8nXwZc"
      },
      "source": [
        "**Removing Recipe IDs from json data which are Not Present in Unique Recipe Ids csv file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0FIsPSxA-C6"
      },
      "outputs": [],
      "source": [
        "for x,y in enumerate(data):\n",
        "  if y['Recipe_id'] not in recipeIdslistStringForm:\n",
        "    data.pop(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOTHz0UpX6PM"
      },
      "source": [
        "**Saving the Generated json file which Contains same Unique Recipes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5B0ePVWVxWx"
      },
      "outputs": [],
      "source": [
        "with open(\"data_v1.json\", \"w\") as final:\n",
        "    json.dump(data, final)\n",
        "files.download('data_v1.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQwH7V5aYF3l"
      },
      "source": [
        "**Pre-Processing Steps Start from Here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C1hkHwVYMCt"
      },
      "source": [
        "**Opening the json file that was Dumped above**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgQVevejkArL"
      },
      "outputs": [],
      "source": [
        "f = open('/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/data_v1.json')\n",
        "data_new = json.load(f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvzd17KCYe16"
      },
      "source": [
        "**Displaying the Total Number of Recipes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FwYgsNcleg4"
      },
      "outputs": [],
      "source": [
        "print(\"Total Number of Recipes are: \",len(data_new))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlWNUuTFYpXE"
      },
      "source": [
        "**Defining the function to perform Ingredients Merging to the corresponding Recipe IDs and their instruction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_mlLNR3Xf_C"
      },
      "outputs": [],
      "source": [
        "def load_dataset(ingredients_path,steps_path, title_path):\n",
        "    print(\"Loading all required files..\\n\")\n",
        "    df_titles = pd.read_csv(title_path)\n",
        "    ingredients = pd.read_csv(ingredients_path)\n",
        "    with open(steps_path) as json_file: \n",
        "        steps = json.load(json_file) \n",
        "\n",
        "    print(\"\\n\\nCreating steps dict..\\n\")\n",
        "    steps_dic = {}\n",
        "    for dic in steps:\n",
        "        steps_dic[int(dic['Recipe_id'])] = dic['steps'].split(';')\n",
        "\n",
        "    print(\"\\n\\nCreating title dict..\\n\")\n",
        "    recipe_ids = []\n",
        "    recipe_ids = df_titles['Recipe_id'].tolist()\n",
        "    titles = df_titles['Recipe_title'].tolist()\n",
        "    continents = df_titles['Continent'].tolist()\n",
        "    regions = df_titles['Region'].tolist()\n",
        "    sub_region = df_titles['Sub_region'].tolist()\n",
        "    title_dic = {}\n",
        "    continet_dict = {}\n",
        "    region_dict = {}\n",
        "    sub_region_dict = {}\n",
        "\n",
        "    for i in range(len(titles)):\n",
        "      if recipe_ids[i] not in title_dic:\n",
        "        title_dic[recipe_ids[i]]=titles[i]\n",
        "        continet_dict[recipe_ids[i]] = continents[i]\n",
        "        region_dict[recipe_ids[i]] = regions[i]\n",
        "        sub_region_dict[recipe_ids[i]] = sub_region[i]\n",
        "    \n",
        "    print(\"\\n\\nCreating ingredients dict..\\n\")\n",
        "    recipe_ids = []\n",
        "    recipe_ids = ingredients['recipe_no'].tolist()\n",
        "    ing = ingredients['ingredient'].tolist()\n",
        "    ing_phrase = ingredients['ingredient_Phrase'].tolist()\n",
        "\n",
        "    ingredient_dic = {}\n",
        "    for i in range(len(recipe_ids)):\n",
        "        ingredient_dic[recipe_ids[i]] = []\n",
        "    for i in range(len(ing)):\n",
        "        if str(ing[i]) != 'nan':\n",
        "            ingredient_dic[recipe_ids[i]].append(ing[i])\n",
        "    \n",
        "    ing_phrase_dic = {}\n",
        "    for i in range(len(recipe_ids)):\n",
        "        ing_phrase_dic[recipe_ids[i]] = []\n",
        "    for i in range(len(ing_phrase)):\n",
        "        if str(ing_phrase[i]) != 'nan':\n",
        "            ing_phrase_dic[recipe_ids[i]].append(ing_phrase[i])\n",
        "\n",
        "    print(\"\\nCreating data and validating..\\n\")\n",
        "    dataset = []\n",
        "    recipe_ids =  list(set(ingredients['recipe_no'].tolist()))\n",
        "    \n",
        "    for i in recipe_ids:\n",
        "      recipe = {}\n",
        "      recipe['ID'] = i\n",
        "      try:\n",
        "        recipe['title'] = title_dic[i]\n",
        "        recipe['ingredients'] = ingredient_dic[i]\n",
        "        recipe['ingredient_phrase'] = ing_phrase_dic[i]\n",
        "        recipe['continent'] = continet_dict[i]\n",
        "        recipe['region'] = region_dict[i]\n",
        "        recipe['sub_region'] = sub_region_dict[i]\n",
        "        recipe['instructions'] = steps_dic[i]\n",
        "\n",
        "      except KeyError:\n",
        "        continue\n",
        "        \n",
        "      if len(recipe['title']) != 0 and len(recipe['instructions']) != 0 and len(recipe['ingredients']) != 0:\n",
        "          dataset.append(recipe)\n",
        "    \n",
        "    print(\"\\n COMPLETED\")\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5pb6rQxZMez"
      },
      "source": [
        "**Defining the Path of the Files required for performing the Merging Operation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXmb3iglX2sC"
      },
      "outputs": [],
      "source": [
        "steps_path = '/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/data_v1.json'\n",
        "titles_path = '/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/Recipes(6).csv'\n",
        "ingredients_path = '/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/RecipeDB_v1/Recipe_correct_ndb_updated_v1.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaWF95zmZcxY"
      },
      "source": [
        "**Calling the above method for Performing the corresponding operation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tErmqBIbYMl2"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(ingredients_path,steps_path, titles_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bagW_rEWZjqD"
      },
      "source": [
        "**Displaying the Single Recipe Data to Analyze and Decide what Pre-Processing steps should be applied on the Recipes Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esRecoK1ZBzD",
        "outputId": "ffc52384-2449-464a-c474-24862a93835b"
      },
      "outputs": [],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuhzNQTdZ9Sk"
      },
      "source": [
        "**Defining Pre-Processing Methods and Applying them on the Recipe Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj-9a4YtbwN_"
      },
      "source": [
        "**Method to Lemmatize the Ingredients of the Recipe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-kW27OWZMEN"
      },
      "outputs": [],
      "source": [
        "def clean_ingredients(l):\n",
        "  l = [ele.lower() for ele in l]\n",
        "  l = [ lemmatizer.lemmatize(ele) for ele in l]\n",
        "  l =  set(l)\n",
        "  l = list(l)\n",
        "  return l\n",
        "\n",
        "for i in range(len(data)):\n",
        "  ing = data[i]['ingredients']\n",
        "  ing_fix = clean_ingredients(ing)\n",
        "  data[i]['ingredients'] = ing_fix "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2cgjj7Xb2tL"
      },
      "source": [
        "**Method to Fix the Punctuation of the Instructions of the Recipes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2YztborZgCZ"
      },
      "outputs": [],
      "source": [
        "def fix_punctuation(l):\n",
        "  newl = []\n",
        "  for i in range(len(l)):\n",
        "    x = re.sub(r'\\s([?.!\",](?:\\s|$))', r'\\1', l[i])\n",
        "    newl.append(x)\n",
        "  return newl\n",
        "\n",
        "punc_fix_data = []\n",
        "for i in range(len(data)):\n",
        "  ins = data[i]['instructions']\n",
        "  ins_fix = fix_punctuation(ins)\n",
        "  data[i]['instructions'] = ins_fix "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVOuX6P2cGqj"
      },
      "source": [
        "**Method to Capitalize and Removing the Extra space at start of the Instructions of the Recipe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R5GxHa6Zj0o"
      },
      "outputs": [],
      "source": [
        "p = re.compile(r'((?<=[\\.\\?!]\\s)(\\w+)|(^\\w+)|(^\\w*))')\n",
        "def cap(match):\n",
        "    return(match.group().capitalize())\n",
        "\n",
        "def fix_caps(l):\n",
        "  newl = []\n",
        "  for i in range(len(l)):\n",
        "    a = l[i].lstrip()\n",
        "    y = p.sub(cap, a)\n",
        "    newl.append(y)\n",
        "  return newl\n",
        "\n",
        "\n",
        "for i in range(len(data)):\n",
        "  ins = data[i]['instructions']\n",
        "  ins_caps = fix_caps(ins)\n",
        "  data[i]['instructions'] = ins_caps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSp8g07Na0Bt"
      },
      "source": [
        "**Displaying the Single Recipe Data after Applying the above Pre-Processing Methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ymvc8tKIZo99",
        "outputId": "ba5db86f-5722-4a2b-99b2-4fbdc32180a1"
      },
      "outputs": [],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFIl3CdDbDdT"
      },
      "source": [
        "**Placing '.' (full stop) after instructions which do not end with the '.' (full stop)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7izAsx_jZuiP"
      },
      "outputs": [],
      "source": [
        "for i in range(len(data)):\n",
        "  ins = data[i]['instructions']\n",
        "  for j in range(len(ins)):\n",
        "    if(ins[j].endswith('.')):\n",
        "      continue\n",
        "    else:\n",
        "      ins[j] = ins[j].rstrip()\n",
        "      ins[j]=ins[j]+\".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwxsCBe3bhca"
      },
      "source": [
        "**Saving the Final Pre-Processed File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6vCddBXZyOd"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/data_v1.pickle', 'wb') as handle:\n",
        "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
