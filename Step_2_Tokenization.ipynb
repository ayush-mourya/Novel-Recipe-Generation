{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "GWZkPIPuNfTy",
      "metadata": {
        "id": "GWZkPIPuNfTy"
      },
      "source": [
        "**Installing & Importing Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v2TsjoGERiZq",
      "metadata": {
        "id": "v2TsjoGERiZq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aRvuXwcgNpew",
      "metadata": {
        "id": "aRvuXwcgNpew"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import h5py\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib as jb\n",
        "from transformers import GPT2Tokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kTts_zswOGQ7",
      "metadata": {
        "id": "kTts_zswOGQ7"
      },
      "source": [
        "**Mounting Google Drive for importing the Data Files which will be used in the Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IGpbSsbJOUu5",
      "metadata": {
        "id": "IGpbSsbJOUu5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qYIm1rnBN4nn",
      "metadata": {
        "id": "qYIm1rnBN4nn"
      },
      "source": [
        "**Importing Updated Recipe json File which contains Recipe Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "weird-allergy",
      "metadata": {
        "id": "weird-allergy"
      },
      "outputs": [],
      "source": [
        "df_new = jb.load('/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/data_v1.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m8pB2zjZObzR",
      "metadata": {
        "id": "m8pB2zjZObzR"
      },
      "source": [
        "**Converting Recipe json File into a DataFrame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8uaLCjDq9cC",
      "metadata": {
        "id": "d8uaLCjDq9cC"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(df_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QrZYQ2tirGZE",
      "metadata": {
        "id": "QrZYQ2tirGZE"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mi5qNn5xOvjc",
      "metadata": {
        "id": "mi5qNn5xOvjc"
      },
      "source": [
        "**Formatting the Instructions of the Recipe by Performing operations like removing '\\t' from the beginning of the instructions, inserting ';' at the end of each instruction, etc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6liGcp0Vz0aL",
      "metadata": {
        "id": "6liGcp0Vz0aL"
      },
      "outputs": [],
      "source": [
        "list_of_instrns = []\n",
        "for row in range(0,len(df)):\n",
        "    instr = df.iloc[row]['instructions']\n",
        "    \n",
        "    strg = \"\"\n",
        "    length = len(instr) - 1\n",
        "    count = 0\n",
        "    for instruction in instr:\n",
        "        processed_instr = []\n",
        "        for j in range(0,len(instruction)):\n",
        "            if(instruction[j]=='|' or instruction[j]=='\\t'):\n",
        "                continue\n",
        "            elif(instruction[j]==' '):\n",
        "                if(instruction[j-1]!='|'):\n",
        "                   strg = strg + instruction[j]\n",
        "            elif(instruction[j] == '.') and (j!=len(instruction)-1) and (instruction[j-1].isdigit()==False):\n",
        "                strg = strg + ' '\n",
        "                strg = strg + instruction[j]      \n",
        "            elif(instruction[j]>='a' and instruction[j]<='z') or (instruction[j]>='A' and instruction[j]<='Z') :\n",
        "                 strg =  strg + instruction[j].lower()\n",
        "            elif(instruction[j] == ','):\n",
        "                  strg =  strg + ' '\n",
        "                  strg =  strg + ',' \n",
        "            elif(instruction[j].isdigit()):\n",
        "                if(instruction[j+1] == '.') or (instruction[j+2] == '.'):\n",
        "                    continue\n",
        "                else:\n",
        "                    strg = strg + instruction[j]  \n",
        "                     \n",
        "        if(count!=length):        \n",
        "            strg = strg + ' '\n",
        "            strg = strg + ';' \n",
        "            strg = strg + ' '\n",
        "  \n",
        "   \n",
        "        count = count + 1     \n",
        "          \n",
        "    processed_instr.append(strg)\n",
        "    list_of_instrns.append(processed_instr)      "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MTRskfyTPZqK",
      "metadata": {
        "id": "MTRskfyTPZqK"
      },
      "source": [
        "**Deleting the current \"instructions\" column from the DataFrame and inserting the modified Instructions by Creating the new \"instructions\" column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thBlxEbU2esx",
      "metadata": {
        "id": "thBlxEbU2esx"
      },
      "outputs": [],
      "source": [
        "df.drop('instructions', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6z0mlsfx2kmz",
      "metadata": {
        "id": "6z0mlsfx2kmz"
      },
      "outputs": [],
      "source": [
        "df['instructions'] = list_of_instrns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FxkhruF-2sll",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "FxkhruF-2sll",
        "outputId": "56b375dc-ba48-4976-baf7-a3d9d2a3dd07"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ziMwJ9oKPttI",
      "metadata": {
        "id": "ziMwJ9oKPttI"
      },
      "source": [
        "**Dividing whole Data into Train and Test part with the Ratio of Train to Test is 0.96 : 0.04**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reserved-heavy",
      "metadata": {
        "id": "reserved-heavy"
      },
      "outputs": [],
      "source": [
        "train,test = train_test_split(df, train_size=0.96, random_state= 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BIo4YvmTQGSl",
      "metadata": {
        "id": "BIo4YvmTQGSl"
      },
      "source": [
        "**Displaying the Size of Train and Test Part and Resetting to the Default Index of these portions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iT38CTW2QL0h",
      "metadata": {
        "id": "iT38CTW2QL0h"
      },
      "outputs": [],
      "source": [
        "print(\"Train Portion size is: \",train.shape)\n",
        "print(\"Test Portion size is: \",test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "retired-allen",
      "metadata": {
        "id": "retired-allen"
      },
      "outputs": [],
      "source": [
        "train.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "liQi9K8dQrQg",
      "metadata": {
        "id": "liQi9K8dQrQg"
      },
      "source": [
        "**Defining the function that will be used for Converting the Dataset into Text Data Format so that the the Data can be Tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "removed-trailer",
      "metadata": {
        "id": "removed-trailer"
      },
      "outputs": [],
      "source": [
        "def df_to_plaintext_file(input_df, output_file):\n",
        "    print(\"Writing to\", output_file)\n",
        "    with open(output_file, 'w', encoding=\"utf-8\") as f:\n",
        "        for index, row in input_df.iterrows():\n",
        "            title = row.title\n",
        "            instructions = row.instructions[0].split('.')[:-1]\n",
        "            ingredients = row.ingredient_phrase\n",
        "            keyword = row.ingredients\n",
        "            \n",
        "            if index%40000==0:\n",
        "                print(index)\n",
        "                print(\"ingreds --->\",ingredients)\n",
        "                print(\"keywords --->\",keyword)\n",
        "\n",
        "            res = \"<RECIPE_START> <INPUT_START> \" + \" <NEXT_INPUT> \".join(keyword) + \" <INPUT_END> <TITLE_START> \" + \\\n",
        "              title + \"<TITLE_END> <INGR_START> \" + \\\n",
        "              \" <NEXT_INGR> \".join(ingredients) + \" <INGR_END> <INSTR_START> \" + \" <NEXT_INSTR> \".join(instructions) + \" <INSTR_END> <RECIPE_END>\"\n",
        "            f.write(\"{}\\n\".format(res))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NO7p7NoCRGCY",
      "metadata": {
        "id": "NO7p7NoCRGCY"
      },
      "source": [
        "**Saving the Processed Train and Test Files to the Custom Path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "historic-scanner",
      "metadata": {
        "id": "historic-scanner"
      },
      "outputs": [],
      "source": [
        "df_to_plaintext_file(train, '/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/train_temp.txt')\n",
        "df_to_plaintext_file(test, '/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/test_temp.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OP_F-goqRz1S",
      "metadata": {
        "id": "OP_F-goqRz1S"
      },
      "source": [
        "**Initializing the GPT2 Tokenizer and Adding special Tokens defined by us to Define the different parts of the Recipe like its title, constituting ingredeints, etc**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "outer-extent",
      "metadata": {
        "id": "outer-extent"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=False)\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M')\n",
        "special_tokens = {\n",
        "    \"additional_special_tokens\": ['<RECIPE_START>',\n",
        "                                  '<INPUT_START>',\n",
        "                                  '<NEXT_INPUT>',\n",
        "                                  '<INPUT_END>',\n",
        "                                  '<INGR_START>',\n",
        "                                  '<NEXT_INGR>',\n",
        "                                  '<INGR_END>',\n",
        "                                  '<INSTR_START>',\n",
        "                                  '<NEXT_INSTR>',\n",
        "                                  '<INSTR_END>',\n",
        "                                  '<TITLE_START>'\n",
        "                                  ,'<TITLE_END>'\n",
        "                                  ,'<RECIPE_END>'\n",
        "    ]\n",
        "}\n",
        "\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "end_token_id = tokenizer.convert_tokens_to_ids([\"<RECIPE_END>\"])[0]\n",
        "\n",
        "hf = h5py.File(\"/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/data_temp.h5\", \"w\")\n",
        "for filename in [\"/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/test_temp\", \"/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/train_temp\"]:\n",
        "    out_np = []\n",
        "    data = open(filename+\".txt\", \"r\")\n",
        "    num = 0\n",
        "    rows = 0\n",
        "    last=[]\n",
        "    for line in data:\n",
        "        num+=1\n",
        "        if num%10000 == 0:\n",
        "            print(\"Read \"+str(num)+\" Written: \"+str(rows))\n",
        "\n",
        "        text_tokens = tokenizer.tokenize(line) \n",
        "        # the tokens supported by gpt2 are 1024 for gpt2 medium. so if the recipe is exceeds this length it wont fit in the model and will generate errors. \n",
        "        if len(text_tokens) > 1024: \n",
        "            continue\n",
        "\n",
        "        text_tokens_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
        "\n",
        "        if (len(last) + len(text_tokens_ids)) <= 1024:\n",
        "            last+=text_tokens_ids\n",
        "        else:\n",
        "            while len(last) < 1024:\n",
        "                last.append(end_token_id)\n",
        "            out_np.append(last)\n",
        "            last=text_tokens_ids\n",
        "            rows+=1\n",
        "    out_mat = np.matrix(out_np)\n",
        "    print(out_mat.shape)\n",
        "    hf.create_dataset(filename, data=out_mat)\n",
        "hf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hd-jXC4BSUBB",
      "metadata": {
        "id": "Hd-jXC4BSUBB"
      },
      "source": [
        "**Displaying the Final Length of Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "genuine-production",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "genuine-production",
        "outputId": "8f625620-8603-4887-a5fd-8a04b802d485"
      },
      "outputs": [],
      "source": [
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4FpGaVCFSfV6",
      "metadata": {
        "id": "4FpGaVCFSfV6"
      },
      "source": [
        "**Displaying the Final Number of Recipes Downsampled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cheap-organ",
      "metadata": {
        "id": "cheap-organ"
      },
      "outputs": [],
      "source": [
        "t = []\n",
        "with open('/content/drive/MyDrive/Monsoon22_conditional_recipe_gen/train_temp.txt') as file1:\n",
        "    for f in file1:\n",
        "        t.append(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usual-technical",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usual-technical",
        "outputId": "ac84d426-dddb-4156-ccae-5794fd42ff5e"
      },
      "outputs": [],
      "source": [
        "print('No of recipes downsampled for prototyping: ',len(t))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.7rc1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7rc1"
    },
    "vscode": {
      "interpreter": {
        "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
