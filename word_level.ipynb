{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pickle\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Ayush Mourya\\OneDrive\\Desktop\\IIITD\\Novel Recipe Generation\\All CSVs\\train_tokenized.csv\", \"r\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(data, columns=[\"Training Data\"])\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Ayush Mourya\\OneDrive\\Desktop\\IIITD\\Novel Recipe Generation\\All CSVs\\test_tokenized.csv\", \"r\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(data, columns=[\"Testing Data\"])\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.iloc[:2]\n",
    "test_df = test_df.iloc[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract recipe texts from the DataFrames as lists\n",
    "train_recipes = train_df['Training Data'].tolist()\n",
    "test_recipes = test_df['Testing Data'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    oov_token='<OOV>',\n",
    "    char_level=False,\n",
    "    filters='',\n",
    "    lower=False,\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(train_recipes)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index['<RECIPE_START>'])\n",
    "print(word_index['<RECIPE_END>\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer for later use\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert recipes to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum sequence length for padding\n",
    "max_sequence_length = max(len(seq) for seq in train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for i in range(len(train_sequences)):\n",
    "    for j in range(1,len(train_sequences[i])):\n",
    "        input_sequences.append(train_sequences[i][:j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to a fixed length\n",
    "train_sequences_padded = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Create Input/Target Pairs for Sequence Generation\n",
    "# -----------------------------\n",
    "\n",
    "# For demonstration, we create a simple sequence-to-sequence prediction\n",
    "# Shift each sequence by one token: predict next word given previous words.\n",
    "\n",
    "X = train_sequences_padded[:,:-1]\n",
    "y = train_sequences_padded[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y,num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Building the LSTM Model\n",
    "# -----------------------------\n",
    "\n",
    "embedding_dim = 256\n",
    "lstm_units = 320\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length-1),\n",
    "    LSTM(lstm_units, return_sequences=False),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Training the Model\n",
    "# -----------------------------\n",
    "\n",
    "# Setup checkpoint callback to save the best model during training\n",
    "checkpoint = ModelCheckpoint('recipe_lstm_model.h5', monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=5, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using pickle\n",
    "with open('recipe_lstm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(seed_text, tokenizer, model, max_sequence_length, num_words=200):\n",
    "    generated_text = seed_text\n",
    "    for _ in range(num_words):\n",
    "        # Tokenize the current seed text\n",
    "        sequence = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "        # Pad the sequence to the required max_sequence_length\n",
    "        sequence = pad_sequences([sequence], maxlen=max_sequence_length, padding='pre')\n",
    "        # Predict the next word\n",
    "        pred = model.predict(sequence, verbose=0)\n",
    "        # Handle 2D output (final timestep prediction)\n",
    "        pred_word_index = np.argmax(pred[0, :])\n",
    "        # If the prediction is padding (index 0), break the loop\n",
    "        if pred_word_index == 0:\n",
    "            break\n",
    "        # Convert the predicted index back to a word\n",
    "        pred_word = tokenizer.index_word.get(pred_word_index, \"\")\n",
    "        # Append the predicted word to the generated text\n",
    "        generated_text += \" \" + pred_word\n",
    "        # Stop if the end token is generated\n",
    "        if pred_word == \"<RECIPE_END>\":\n",
    "            break\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example seed text from the test dataset (the INPUT part)\n",
    "seed_text = \"<RECIPE_START> <INPUT_START> mustard <NEXT_INPUT> clove garlic <NEXT_INPUT> sugar <NEXT_INPUT> corn oil <NEXT_INPUT> salt <NEXT_INPUT> mayonnaise <NEXT_INPUT> red wine vinegar <NEXT_INPUT> paprika <INPUT_END>\"\n",
    "\n",
    "generated_recipe = generate_recipe(seed_text, tokenizer, model, max_sequence_length, num_words=200)\n",
    "print(\"Generated Recipe:\\n\", generated_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7. Extracting Test Recipes and Computing BLEU Score\n",
    "# -----------------------------\n",
    "\n",
    "def extract_input_section(recipe):\n",
    "    \"\"\"\n",
    "    Extract the input portion (from <INPUT_START> to <INPUT_END>) from a recipe.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(<INPUT_START>.*?<INPUT_END>)', recipe, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the input sections from the test set that match the given pattern\n",
    "test_inputs = [extract_input_section(recipe) for recipe in test_recipes if extract_input_section(recipe)]\n",
    "print(\"Number of test recipes with INPUT section:\", len(test_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BLEU score, we assume one reference per generated recipe.\n",
    "# Here, for demonstration, we pick one reference (for example, the first one) from test_inputs.\n",
    "# In practice, you might iterate over multiple recipes and average the BLEU scores.\n",
    "if test_inputs:\n",
    "    reference = test_inputs[0].split()  # Tokenize the reference text\n",
    "    generated_tokens = generated_recipe.split()  # Tokenize the generated recipe\n",
    "\n",
    "    bleu_score = sentence_bleu([reference], generated_tokens)\n",
    "    print(\"BLEU score:\", bleu_score)\n",
    "else:\n",
    "    print(\"No test recipe inputs were extracted for BLEU score computation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
