{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Ayush Mourya\\OneDrive\\Desktop\\IIITD\\Novel Recipe Generation\\All CSVs\\train_tokenized.csv\", \"r\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Training Data\"])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import(\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments)\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {\n",
    "    \"additional_special_tokens\": ['<RECIPE_START>',\n",
    "                                  '<INPUT_START>',\n",
    "                                  '<NEXT_INPUT>',\n",
    "                                  '<INPUT_END>',\n",
    "                                  '<INGR_START>',\n",
    "                                  '<NEXT_INGR>',\n",
    "                                  '<INGR_END>',\n",
    "                                  '<INSTR_START>',\n",
    "                                  '<NEXT_INSTR>',\n",
    "                                  '<INSTR_END>',\n",
    "                                  '<TITLE_START>',\n",
    "                                  '<TITLE_END>',\n",
    "                                  '<RECIPE_END>',\n",
    "                                  '<PAD>'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "model.to(device)\n",
    "\n",
    "# add a pad token to the tokenizer \n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataset = df[\"Training Data\"].apply(lambda x: tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorized_dataset[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([198]))\n",
    "print(tokenizer.convert_ids_to_tokens([50270]))\n",
    "print(tokenizer.convert_ids_to_tokens([50256]))\n",
    "print(tokenizer.convert_tokens_to_ids(\"<RECIPE_END>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_len_list = df.apply(lambda x: len(tokenizer.encode(x[\"Training Data\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_len_list.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "axes[0].boxplot(recipe_len_list)\n",
    "axes[1].hist(recipe_len_list, bins=100)\n",
    "sm.qqplot(recipe_len_list, line='s', ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the length of the tokenized data is greater than 320 or less than 100\n",
    "df_filtered = df[~df[\"Training Data\"].apply(lambda x: len(tokenizer.encode(x)) > 320 or len(tokenizer.encode(x)) < 100)]\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_dataset_filtered = vectorized_dataset[~vectorized_dataset.apply(lambda x: len(x) > 320 or len(x) < 100)]\n",
    "print(vectorized_dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_len_list = df_filtered.apply(lambda x: len(tokenizer.encode(x[\"Training Data\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_len_list.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "axes[0].boxplot(recipe_len_list)\n",
    "axes[1].hist(recipe_len_list, bins=100)\n",
    "sm.qqplot(recipe_len_list, line='s', ax=axes[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df\n",
    "#del vectorized_dataset\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pandas as pd\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_add_eos(recipe, max_length=322, pad_token_id=50270, eos_token_id=50256):\n",
    "    recipe = recipe[:max_length-2]  # Reserve space for EOS token\n",
    "    recipe.append(eos_token_id)  # Add EOS token at the end\n",
    "    padding_length = max_length - len(recipe)\n",
    "    return [pad_token_id] * padding_length + recipe  # Pad on the left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the vectorized dataset\n",
    "vectorized_dataset_padded = vectorized_dataset_filtered.apply(lambda x: pad_and_add_eos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorized_dataset_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, vectorized_data):\n",
    "        self.vectorized_data = vectorized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vectorized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.vectorized_data.iloc[idx]  # Direct indexing if it's a list\n",
    "        #print(data)\n",
    "        return torch.tensor(data)\n",
    "\n",
    "dataset = RecipeDataset(vectorized_dataset_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"/kaggle/input/muah-muah/test_tokenized.csv\", \"r\") as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "test_df = pd.DataFrame(data, columns=[\"Testing Data\"])\n",
    "test_df = test_df.iloc[:4000]\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract input portion for recipe generation\n",
    "def extract_inputs(recipe_text):\n",
    "    start_idx = recipe_text.find(\"<RECIPE_START>\")\n",
    "    end_idx = recipe_text.find(\"<INPUT_END>\") + len(\"<INPUT_END>\")\n",
    "    return recipe_text[start_idx:end_idx+1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract inputs from test dataset\n",
    "test_inputs = test_df[\"Testing Data\"].apply(extract_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, tokenizer, input_text, max_length=150):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            pad_token_id=50270, \n",
    "            eos_token_id=tokenizer.eos_token_id,  \n",
    "            repetition_penalty=1.2,  \n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute BLEU score\n",
    "def compute_bleu_score(generated_recipes, reference_recipes):\n",
    "    scores = []\n",
    "    for gen, ref in zip(generated_recipes, reference_recipes):\n",
    "        reference = [ref.split()]  # Tokenize reference\n",
    "        candidate = gen.split()  # Tokenize generated recipe    \n",
    "        score = sentence_bleu(reference, candidate)\n",
    "        scores.append(score)\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,  # Change to desired epochs\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=50000,  # Save checkpoint every 1000 steps\n",
    "    #save_total_limit=2,\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_strategy=\"epoch\",  # Log loss at each epoch\n",
    "    evaluation_strategy=\"no\",  # Evaluate model at each epoch\n",
    "    report_to=[\"tensorboard\"],  # Enables TensorBoard logging\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training and evaluate after every epoch\n",
    "for epoch in range(num_epochs):\n",
    "    trainer.train()\n",
    "    \n",
    "    # Generate recipes for test set\n",
    "    generated_recipes = []\n",
    "    count = 0\n",
    "    for inp in test_inputs:\n",
    "        count += 1\n",
    "        generated_recipes.append(generate_recipe(model,tokenizer,inp))\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "            \n",
    "    #generated_recipes = [generate_recipe(model, tokenizer, inp) for inp in test_inputs]\n",
    "    #print(generated_recipes)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_score = compute_bleu_score(generated_recipes, test_df[\"Testing Data\"].tolist())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "    # Save the model and tokenizer after each epoch\n",
    "    save_path = f\"./saved_model_epoch_{epoch+1}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"Model saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.state.log_history\n",
    "df = pd.DataFrame(history)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "epoch_to_load = 2  # Change this to the desired epoch\n",
    "load_path = f\"./saved_model_epoch_{epoch_to_load}\"\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(load_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(load_path)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from {load_path}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, tokenizer, input_text, max_length=150):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            pad_token_id=50270, \n",
    "            eos_token_id=tokenizer.eos_token_id,  \n",
    "            repetition_penalty=1.2,  \n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "test_input = \"Make a spicy pasta recipe.\"\n",
    "generated_recipe = generate_recipe(model, tokenizer, test_input)\n",
    "print(\"Generated Recipe:\", generated_recipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Choose which epoch to resume from\n",
    "resume_epoch = 2  # Change this to the desired epoch\n",
    "load_path = f\"./saved_model_epoch_{resume_epoch}\"\n",
    "\n",
    "# Load the model and tokenizer from the saved checkpoint\n",
    "model = AutoModel.from_pretrained(load_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "\n",
    "print(f\"Resuming training from {load_path}\")\n",
    "\n",
    "# Reinitialize Trainer with the loaded model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5 - resume_epoch,  # Continue from where it left off\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=50000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Continue training from the next epoch\n",
    "for epoch in range(resume_epoch, training_args.num_train_epochs + resume_epoch):\n",
    "    trainer.train()\n",
    "    \n",
    "    # Generate recipes for test set\n",
    "    generated_recipes = [generate_recipe(model, tokenizer, inp) for inp in test_inputs]\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_score = compute_bleu_score(generated_recipes, test_df[\"testing_data\"])\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} - BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "    # Save model again\n",
    "    save_path = f\"./saved_model_epoch_{epoch+1}\"\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"Model saved at {save_path}\")'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
